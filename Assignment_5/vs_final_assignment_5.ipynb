{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vs-final-assignment-5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "6c93d84d-5add-4af9-b1df-4a8328374340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/colab_data/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "outputId": "78a3c3a3-c0f7-44a8-87fd-32094c273acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "a53ba12c-8e1c-44f7-c7c6-49d97312e072",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "e63d30b4-c3f8-43b6-ef79-6ac522993adb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "dacd7ed2-1aa5-4804-8ace-522bd1ee0100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "9c0b1214-15c9-45f4-bf81-bf1fcd783818",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5297</th>\n",
              "      <td>resized/5298.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5726</th>\n",
              "      <td>resized/5727.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>resized/340.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10522</th>\n",
              "      <td>resized/10524.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12455</th>\n",
              "      <td>resized/12457.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "5297    resized/5298.jpg              1  ...                        1              0\n",
              "5726    resized/5727.jpg              0  ...                        0              1\n",
              "339      resized/340.jpg              1  ...                        1              0\n",
              "10522  resized/10524.jpg              0  ...                        1              0\n",
              "12455  resized/12457.jpg              0  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32)\n",
        "valid_gen = PersonDataGenerator(train_df, batch_size=32, shuffle=False)\n",
        "test_gen = PersonDataGenerator(val_df, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "3d6e1506-2ca6-4963-d75a-bacb2877a05f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backbone = VGG16(\n",
        "    weights=None, \n",
        "    include_top=False, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "neck = backbone.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = BatchNormalization()(in_layer)\n",
        "    neck = Dropout(0.1)(neck)\n",
        "    neck = Dense(256, activation=\"relu\")(neck)\n",
        "    neck = BatchNormalization()(neck)\n",
        "    neck = Dropout(0.1)(neck)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=2, min_lr=0.1e-6)\n",
        "early_stopper = EarlyStopping(min_delta=0.001, patience=5, restore_best_weights=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWVxcbi_y6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze backbone\n",
        "for layer in backbone.layers:\n",
        "\tlayer.trainable = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWxZN8IV6xWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "\n",
        "class LRFinder(Callback):\n",
        "    \n",
        "    '''\n",
        "    A simple callback for finding the optimal learning rate range for your model + dataset. \n",
        "    \n",
        "    # Usage\n",
        "        ```python\n",
        "            lr_finder = LRFinder(min_lr=1e-5, \n",
        "                                 max_lr=1e-2, \n",
        "                                 steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
        "                                 epochs=3)\n",
        "            model.fit(X_train, Y_train, callbacks=[lr_finder])\n",
        "            \n",
        "            lr_finder.plot_loss()\n",
        "        ```\n",
        "    \n",
        "    # Arguments\n",
        "        min_lr: The lower bound of the learning rate range for the experiment.\n",
        "        max_lr: The upper bound of the learning rate range for the experiment.\n",
        "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
        "        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n",
        "        \n",
        "    # References\n",
        "        Blog post: jeremyjordan.me/nn-learning-rate\n",
        "        Original paper: https://arxiv.org/abs/1506.01186\n",
        "\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.total_iterations = steps_per_epoch * epochs\n",
        "        self.iteration = 0\n",
        "        self.history = {}\n",
        "        \n",
        "    def clr(self):\n",
        "        '''Calculate the learning rate.'''\n",
        "        x = self.iteration / self.total_iterations \n",
        "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
        "        \n",
        "    def on_train_begin(self, logs=None):\n",
        "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
        "        logs = logs or {}\n",
        "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
        "        \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        '''Record previous batch statistics and update the learning rate.'''\n",
        "        logs = logs or {}\n",
        "        self.iteration += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.iteration)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "            \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        " \n",
        "    def plot_lr(self):\n",
        "        '''Helper function to quickly inspect the learning rate schedule.'''\n",
        "        plt.plot(self.history['iterations'], self.history['lr'])\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Learning rate')\n",
        "        plt.show()\n",
        "        \n",
        "    def plot_loss(self):\n",
        "        '''Helper function to quickly observe the learning rate experiment results.'''\n",
        "        plt.plot(self.history['lr'], self.history['loss'])\n",
        "        plt.xscale('log')\n",
        "        plt.xlabel('Learning rate')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg5H5t1W6-ZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10):\n",
        "    '''\n",
        "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
        "    '''\n",
        "    def schedule(epoch):\n",
        "        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
        "    \n",
        "    return LearningRateScheduler(schedule)\n",
        "\n",
        "lr_sched = step_decay_schedule(initial_lr=1e-4, decay_factor=0.75, step_size=2)\n",
        "\n",
        "# model.fit(X_train, Y_train, callbacks=[lr_sched])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = {\n",
        "\t\"gender_output\": \"binary_crossentropy\",\n",
        "\t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "\t\"age_output\": \"categorical_crossentropy\",\n",
        "\t\"weight_output\": \"categorical_crossentropy\",\n",
        "    \"bag_output\": \"categorical_crossentropy\",\n",
        "    \"footwear_output\": \"categorical_crossentropy\",\n",
        "    \"emotion_output\": \"categorical_crossentropy\",\n",
        "    \"pose_output\": \"categorical_crossentropy\"\n",
        "}\n",
        "\n",
        "loss_weights = {\n",
        "    \"gender_output\": 0.9, \n",
        "    \"image_quality_output\": 0.2, \n",
        "    \"age_output\": 0.4,\n",
        "    \"weight_output\": 0.7,\n",
        "    \"bag_output\": 0.5,\n",
        "    \"footwear_output\": 0.6,\n",
        "    \"emotion_output\": 0.1,\n",
        "    \"pose_output\": 0.3\n",
        "}\n",
        "\n",
        "# opt = SGD(lr=1e-3, momentum=0.9, nesterov=False, decay=1e-6)\n",
        "model.compile(\n",
        "    optimizer=\"SGD\",\n",
        "    loss=losses, \n",
        "    loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlKNcl097A4y",
        "colab_type": "code",
        "outputId": "733b1133-eb5f-43b5-e5f2-4cafe0a40e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history= model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=[lr_sched]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "360/360 [==============================] - 149s 414ms/step - loss: 4.5309 - gender_output_loss: 0.8748 - image_quality_output_loss: 1.2815 - age_output_loss: 1.7730 - weight_output_loss: 1.4496 - bag_output_loss: 1.1535 - footwear_output_loss: 1.1140 - pose_output_loss: 1.1258 - emotion_output_loss: 1.8051 - gender_output_acc: 0.5351 - image_quality_output_acc: 0.3775 - age_output_acc: 0.2574 - weight_output_acc: 0.3562 - bag_output_acc: 0.5122 - footwear_output_acc: 0.4705 - pose_output_acc: 0.4958 - emotion_output_acc: 0.2042 - val_loss: 4.3120 - val_gender_output_loss: 0.8607 - val_image_quality_output_loss: 1.2294 - val_age_output_loss: 1.7759 - val_weight_output_loss: 1.2673 - val_bag_output_loss: 1.1446 - val_footwear_output_loss: 1.0994 - val_pose_output_loss: 1.0766 - val_emotion_output_loss: 1.3917 - val_gender_output_acc: 0.6005 - val_image_quality_output_acc: 0.4297 - val_age_output_acc: 0.2672 - val_weight_output_acc: 0.4607 - val_bag_output_acc: 0.5128 - val_footwear_output_acc: 0.5058 - val_pose_output_acc: 0.5707 - val_emotion_output_acc: 0.3943\n",
            "Epoch 2/50\n",
            "360/360 [==============================] - 128s 355ms/step - loss: 4.0378 - gender_output_loss: 0.7401 - image_quality_output_loss: 1.1899 - age_output_loss: 1.6752 - weight_output_loss: 1.1971 - bag_output_loss: 1.0620 - footwear_output_loss: 1.0287 - pose_output_loss: 1.0743 - emotion_output_loss: 1.5512 - gender_output_acc: 0.5770 - image_quality_output_acc: 0.4099 - age_output_acc: 0.2861 - weight_output_acc: 0.4951 - bag_output_acc: 0.5178 - footwear_output_acc: 0.5114 - pose_output_acc: 0.5231 - emotion_output_acc: 0.2867 - val_loss: 3.9486 - val_gender_output_loss: 0.6938 - val_image_quality_output_loss: 1.1810 - val_age_output_loss: 1.6006 - val_weight_output_loss: 1.2371 - val_bag_output_loss: 1.0705 - val_footwear_output_loss: 0.9845 - val_pose_output_loss: 1.0824 - val_emotion_output_loss: 1.3101 - val_gender_output_acc: 0.6114 - val_image_quality_output_acc: 0.3650 - val_age_output_acc: 0.3478 - val_weight_output_acc: 0.4747 - val_bag_output_acc: 0.5173 - val_footwear_output_acc: 0.5020 - val_pose_output_acc: 0.5016 - val_emotion_output_acc: 0.4150\n",
            "Epoch 3/50\n",
            "360/360 [==============================] - 128s 356ms/step - loss: 3.8949 - gender_output_loss: 0.7104 - image_quality_output_loss: 1.1585 - age_output_loss: 1.6330 - weight_output_loss: 1.1373 - bag_output_loss: 1.0292 - footwear_output_loss: 1.0018 - pose_output_loss: 1.0492 - emotion_output_loss: 1.4401 - gender_output_acc: 0.5962 - image_quality_output_acc: 0.4197 - age_output_acc: 0.3097 - weight_output_acc: 0.5448 - bag_output_acc: 0.5293 - footwear_output_acc: 0.5293 - pose_output_acc: 0.5484 - emotion_output_acc: 0.3490 - val_loss: 3.6765 - val_gender_output_loss: 0.7018 - val_image_quality_output_loss: 1.0783 - val_age_output_loss: 1.5423 - val_weight_output_loss: 1.0369 - val_bag_output_loss: 0.9757 - val_footwear_output_loss: 0.9416 - val_pose_output_loss: 0.9880 - val_emotion_output_loss: 1.3731 - val_gender_output_acc: 0.5985 - val_image_quality_output_acc: 0.4648 - val_age_output_acc: 0.3395 - val_weight_output_acc: 0.6074 - val_bag_output_acc: 0.5082 - val_footwear_output_acc: 0.5576 - val_pose_output_acc: 0.5969 - val_emotion_output_acc: 0.3563\n",
            "Epoch 4/50\n",
            "360/360 [==============================] - 128s 355ms/step - loss: 3.8295 - gender_output_loss: 0.6971 - image_quality_output_loss: 1.1422 - age_output_loss: 1.6090 - weight_output_loss: 1.1066 - bag_output_loss: 1.0158 - footwear_output_loss: 0.9970 - pose_output_loss: 1.0445 - emotion_output_loss: 1.3602 - gender_output_acc: 0.6062 - image_quality_output_acc: 0.4288 - age_output_acc: 0.3208 - weight_output_acc: 0.5605 - bag_output_acc: 0.5308 - footwear_output_acc: 0.5299 - pose_output_acc: 0.5447 - emotion_output_acc: 0.4049 - val_loss: 3.5918 - val_gender_output_loss: 0.6261 - val_image_quality_output_loss: 1.0683 - val_age_output_loss: 1.5274 - val_weight_output_loss: 1.0394 - val_bag_output_loss: 0.9641 - val_footwear_output_loss: 0.9294 - val_pose_output_loss: 0.9948 - val_emotion_output_loss: 1.3803 - val_gender_output_acc: 0.6543 - val_image_quality_output_acc: 0.4560 - val_age_output_acc: 0.3437 - val_weight_output_acc: 0.5797 - val_bag_output_acc: 0.5464 - val_footwear_output_acc: 0.5515 - val_pose_output_acc: 0.5347 - val_emotion_output_acc: 0.3286\n",
            "Epoch 5/50\n",
            "360/360 [==============================] - 128s 355ms/step - loss: 3.7689 - gender_output_loss: 0.6760 - image_quality_output_loss: 1.1235 - age_output_loss: 1.5939 - weight_output_loss: 1.0903 - bag_output_loss: 1.0061 - footwear_output_loss: 0.9855 - pose_output_loss: 1.0290 - emotion_output_loss: 1.3195 - gender_output_acc: 0.6215 - image_quality_output_acc: 0.4418 - age_output_acc: 0.3269 - weight_output_acc: 0.5693 - bag_output_acc: 0.5299 - footwear_output_acc: 0.5392 - pose_output_acc: 0.5530 - emotion_output_acc: 0.4233 - val_loss: 3.4975 - val_gender_output_loss: 0.6123 - val_image_quality_output_loss: 1.0581 - val_age_output_loss: 1.4949 - val_weight_output_loss: 1.0262 - val_bag_output_loss: 0.9448 - val_footwear_output_loss: 0.9179 - val_pose_output_loss: 0.9682 - val_emotion_output_loss: 1.0494 - val_gender_output_acc: 0.6682 - val_image_quality_output_acc: 0.4600 - val_age_output_acc: 0.3373 - val_weight_output_acc: 0.6153 - val_bag_output_acc: 0.5365 - val_footwear_output_acc: 0.5793 - val_pose_output_acc: 0.5729 - val_emotion_output_acc: 0.6703\n",
            "Epoch 6/50\n",
            "360/360 [==============================] - 127s 354ms/step - loss: 3.7170 - gender_output_loss: 0.6678 - image_quality_output_loss: 1.1267 - age_output_loss: 1.5612 - weight_output_loss: 1.0760 - bag_output_loss: 0.9919 - footwear_output_loss: 0.9741 - pose_output_loss: 1.0152 - emotion_output_loss: 1.2798 - gender_output_acc: 0.6313 - image_quality_output_acc: 0.4407 - age_output_acc: 0.3403 - weight_output_acc: 0.5798 - bag_output_acc: 0.5414 - footwear_output_acc: 0.5396 - pose_output_acc: 0.5525 - emotion_output_acc: 0.4528 - val_loss: 3.5153 - val_gender_output_loss: 0.6338 - val_image_quality_output_loss: 1.0321 - val_age_output_loss: 1.4977 - val_weight_output_loss: 1.0376 - val_bag_output_loss: 0.9367 - val_footwear_output_loss: 0.9057 - val_pose_output_loss: 0.9398 - val_emotion_output_loss: 1.1931 - val_gender_output_acc: 0.6423 - val_image_quality_output_acc: 0.5106 - val_age_output_acc: 0.3487 - val_weight_output_acc: 0.5904 - val_bag_output_acc: 0.5546 - val_footwear_output_acc: 0.5850 - val_pose_output_acc: 0.6021 - val_emotion_output_acc: 0.5265\n",
            "Epoch 7/50\n",
            "360/360 [==============================] - 128s 356ms/step - loss: 3.6892 - gender_output_loss: 0.6576 - image_quality_output_loss: 1.1161 - age_output_loss: 1.5735 - weight_output_loss: 1.0667 - bag_output_loss: 0.9842 - footwear_output_loss: 0.9610 - pose_output_loss: 1.0156 - emotion_output_loss: 1.2468 - gender_output_acc: 0.6361 - image_quality_output_acc: 0.4472 - age_output_acc: 0.3341 - weight_output_acc: 0.5871 - bag_output_acc: 0.5424 - footwear_output_acc: 0.5469 - pose_output_acc: 0.5530 - emotion_output_acc: 0.4766 - val_loss: 3.4132 - val_gender_output_loss: 0.6030 - val_image_quality_output_loss: 1.0272 - val_age_output_loss: 1.4560 - val_weight_output_loss: 0.9967 - val_bag_output_loss: 0.9178 - val_footwear_output_loss: 0.8946 - val_pose_output_loss: 0.9412 - val_emotion_output_loss: 1.0701 - val_gender_output_acc: 0.6685 - val_image_quality_output_acc: 0.4911 - val_age_output_acc: 0.3753 - val_weight_output_acc: 0.6227 - val_bag_output_acc: 0.5570 - val_footwear_output_acc: 0.5913 - val_pose_output_acc: 0.5963 - val_emotion_output_acc: 0.6536\n",
            "Epoch 8/50\n",
            "360/360 [==============================] - 128s 355ms/step - loss: 3.6596 - gender_output_loss: 0.6508 - image_quality_output_loss: 1.1144 - age_output_loss: 1.5647 - weight_output_loss: 1.0596 - bag_output_loss: 0.9796 - footwear_output_loss: 0.9543 - pose_output_loss: 0.9968 - emotion_output_loss: 1.2183 - gender_output_acc: 0.6482 - image_quality_output_acc: 0.4533 - age_output_acc: 0.3384 - weight_output_acc: 0.5930 - bag_output_acc: 0.5435 - footwear_output_acc: 0.5611 - pose_output_acc: 0.5693 - emotion_output_acc: 0.5083 - val_loss: 3.3964 - val_gender_output_loss: 0.5853 - val_image_quality_output_loss: 1.0221 - val_age_output_loss: 1.4651 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.9237 - val_footwear_output_loss: 0.8854 - val_pose_output_loss: 0.9400 - val_emotion_output_loss: 1.1517 - val_gender_output_acc: 0.6884 - val_image_quality_output_acc: 0.5021 - val_age_output_acc: 0.3863 - val_weight_output_acc: 0.6238 - val_bag_output_acc: 0.5509 - val_footwear_output_acc: 0.5983 - val_pose_output_acc: 0.5784 - val_emotion_output_acc: 0.5628\n",
            "Epoch 9/50\n",
            "360/360 [==============================] - 128s 356ms/step - loss: 3.6358 - gender_output_loss: 0.6435 - image_quality_output_loss: 1.1005 - age_output_loss: 1.5505 - weight_output_loss: 1.0585 - bag_output_loss: 0.9749 - footwear_output_loss: 0.9499 - pose_output_loss: 0.9891 - emotion_output_loss: 1.2127 - gender_output_acc: 0.6517 - image_quality_output_acc: 0.4629 - age_output_acc: 0.3416 - weight_output_acc: 0.5972 - bag_output_acc: 0.5514 - footwear_output_acc: 0.5613 - pose_output_acc: 0.5734 - emotion_output_acc: 0.5155 - val_loss: 3.3644 - val_gender_output_loss: 0.5666 - val_image_quality_output_loss: 1.0420 - val_age_output_loss: 1.4519 - val_weight_output_loss: 0.9937 - val_bag_output_loss: 0.9083 - val_footwear_output_loss: 0.8814 - val_pose_output_loss: 0.9256 - val_emotion_output_loss: 1.0914 - val_gender_output_acc: 0.7030 - val_image_quality_output_acc: 0.4591 - val_age_output_acc: 0.3905 - val_weight_output_acc: 0.6354 - val_bag_output_acc: 0.5637 - val_footwear_output_acc: 0.5967 - val_pose_output_acc: 0.6065 - val_emotion_output_acc: 0.6403\n",
            "Epoch 10/50\n",
            "360/360 [==============================] - 128s 356ms/step - loss: 3.6163 - gender_output_loss: 0.6309 - image_quality_output_loss: 1.1051 - age_output_loss: 1.5466 - weight_output_loss: 1.0515 - bag_output_loss: 0.9719 - footwear_output_loss: 0.9510 - pose_output_loss: 0.9850 - emotion_output_loss: 1.2069 - gender_output_acc: 0.6621 - image_quality_output_acc: 0.4563 - age_output_acc: 0.3477 - weight_output_acc: 0.5958 - bag_output_acc: 0.5457 - footwear_output_acc: 0.5651 - pose_output_acc: 0.5709 - emotion_output_acc: 0.5180 - val_loss: 3.3690 - val_gender_output_loss: 0.5808 - val_image_quality_output_loss: 1.0182 - val_age_output_loss: 1.4624 - val_weight_output_loss: 0.9877 - val_bag_output_loss: 0.9077 - val_footwear_output_loss: 0.8752 - val_pose_output_loss: 0.9357 - val_emotion_output_loss: 1.0660 - val_gender_output_acc: 0.6890 - val_image_quality_output_acc: 0.5219 - val_age_output_acc: 0.3708 - val_weight_output_acc: 0.6332 - val_bag_output_acc: 0.5718 - val_footwear_output_acc: 0.6024 - val_pose_output_acc: 0.5852 - val_emotion_output_acc: 0.6511\n",
            "Epoch 11/50\n",
            "Epoch 10/50\n",
            "360/360 [==============================] - 128s 355ms/step - loss: 3.5974 - gender_output_loss: 0.6265 - image_quality_output_loss: 1.1024 - age_output_loss: 1.5436 - weight_output_loss: 1.0479 - bag_output_loss: 0.9598 - footwear_output_loss: 0.9457 - pose_output_loss: 0.9868 - emotion_output_loss: 1.1872 - gender_output_acc: 0.6645 - image_quality_output_acc: 0.4550 - age_output_acc: 0.3537 - weight_output_acc: 0.6007 - bag_output_acc: 0.5490 - footwear_output_acc: 0.5575 - pose_output_acc: 0.5729 - emotion_output_acc: 0.5378 - val_loss: 3.3426 - val_gender_output_loss: 0.5769 - val_image_quality_output_loss: 1.0162 - val_age_output_loss: 1.4399 - val_weight_output_loss: 0.9766 - val_bag_output_loss: 0.8966 - val_footwear_output_loss: 0.8785 - val_pose_output_loss: 0.9183 - val_emotion_output_loss: 1.0975 - val_gender_output_acc: 0.6944 - val_image_quality_output_acc: 0.5030 - val_age_output_acc: 0.3882 - val_weight_output_acc: 0.6359 - val_bag_output_acc: 0.5725 - val_footwear_output_acc: 0.6002 - val_pose_output_acc: 0.6030 - val_emotion_output_acc: 0.6238\n",
            "Epoch 12/50\n",
            "360/360 [==============================] - 128s 356ms/step - loss: 3.5753 - gender_output_loss: 0.6204 - image_quality_output_loss: 1.1010 - age_output_loss: 1.5369 - weight_output_loss: 1.0447 - bag_output_loss: 0.9637 - footwear_output_loss: 0.9294 - pose_output_loss: 0.9757 - emotion_output_loss: 1.1841 - gender_output_acc: 0.6698 - image_quality_output_acc: 0.4605 - age_output_acc: 0.3489 - weight_output_acc: 0.5977 - bag_output_acc: 0.5505 - footwear_output_acc: 0.5736 - pose_output_acc: 0.5788 - emotion_output_acc: 0.5403 - val_loss: 3.3116 - val_gender_output_loss: 0.5604 - val_image_quality_output_loss: 1.0069 - val_age_output_loss: 1.4529 - val_weight_output_loss: 0.9709 - val_bag_output_loss: 0.8915 - val_footwear_output_loss: 0.8649 - val_pose_output_loss: 0.9096 - val_emotion_output_loss: 1.0752 - val_gender_output_acc: 0.7084 - val_image_quality_output_acc: 0.5322 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6312 - val_bag_output_acc: 0.5792 - val_footwear_output_acc: 0.6088 - val_pose_output_acc: 0.6065 - val_emotion_output_acc: 0.6455\n",
            "Epoch 13/50\n",
            "Epoch 12/50\n",
            "360/360 [==============================] - 128s 356ms/step - loss: 3.5697 - gender_output_loss: 0.6230 - image_quality_output_loss: 1.0813 - age_output_loss: 1.5391 - weight_output_loss: 1.0435 - bag_output_loss: 0.9597 - footwear_output_loss: 0.9261 - pose_output_loss: 0.9777 - emotion_output_loss: 1.1780 - gender_output_acc: 0.6699 - image_quality_output_acc: 0.4694 - age_output_acc: 0.3521 - weight_output_acc: 0.6020 - bag_output_acc: 0.5551 - footwear_output_acc: 0.5728 - pose_output_acc: 0.5764 - emotion_output_acc: 0.5398 - val_loss: 3.3059 - val_gender_output_loss: 0.5521 - val_image_quality_output_loss: 1.0088 - val_age_output_loss: 1.4472 - val_weight_output_loss: 0.9712 - val_bag_output_loss: 0.8927 - val_footwear_output_loss: 0.8659 - val_pose_output_loss: 0.9131 - val_emotion_output_loss: 1.0875 - val_gender_output_acc: 0.7147 - val_image_quality_output_acc: 0.5177 - val_age_output_acc: 0.3889 - val_weight_output_acc: 0.6359 - val_bag_output_acc: 0.5810 - val_footwear_output_acc: 0.6070 - val_pose_output_acc: 0.5991 - val_emotion_output_acc: 0.6323\n",
            "Epoch 14/50\n",
            "360/360 [==============================] - 128s 355ms/step - loss: 3.5763 - gender_output_loss: 0.6195 - image_quality_output_loss: 1.0884 - age_output_loss: 1.5317 - weight_output_loss: 1.0482 - bag_output_loss: 0.9608 - footwear_output_loss: 0.9344 - pose_output_loss: 0.9835 - emotion_output_loss: 1.1850 - gender_output_acc: 0.6704 - image_quality_output_acc: 0.4701 - age_output_acc: 0.3563 - weight_output_acc: 0.6005 - bag_output_acc: 0.5564 - footwear_output_acc: 0.5686 - pose_output_acc: 0.5756 - emotion_output_acc: 0.5400 - val_loss: 3.2804 - val_gender_output_loss: 0.5490 - val_image_quality_output_loss: 1.0038 - val_age_output_loss: 1.4319 - val_weight_output_loss: 0.9667 - val_bag_output_loss: 0.8886 - val_footwear_output_loss: 0.8569 - val_pose_output_loss: 0.9135 - val_emotion_output_loss: 1.0353 - val_gender_output_acc: 0.7196 - val_image_quality_output_acc: 0.5278 - val_age_output_acc: 0.3881 - val_weight_output_acc: 0.6328 - val_bag_output_acc: 0.5801 - val_footwear_output_acc: 0.6155 - val_pose_output_acc: 0.5951 - val_emotion_output_acc: 0.6766\n",
            "Epoch 15/50\n",
            "360/360 [==============================] - 128s 355ms/step - loss: 3.5523 - gender_output_loss: 0.6105 - image_quality_output_loss: 1.0941 - age_output_loss: 1.5348 - weight_output_loss: 1.0330 - bag_output_loss: 0.9579 - footwear_output_loss: 0.9313 - pose_output_loss: 0.9732 - emotion_output_loss: 1.1734 - gender_output_acc: 0.6796 - image_quality_output_acc: 0.4635 - age_output_acc: 0.3468 - weight_output_acc: 0.6069 - bag_output_acc: 0.5486 - footwear_output_acc: 0.5720 - pose_output_acc: 0.5768 - emotion_output_acc: 0.5493 - val_loss: 3.2768 - val_gender_output_loss: 0.5513 - val_image_quality_output_loss: 1.0096 - val_age_output_loss: 1.4304 - val_weight_output_loss: 0.9646 - val_bag_output_loss: 0.8852 - val_footwear_output_loss: 0.8528 - val_pose_output_loss: 0.9087 - val_emotion_output_loss: 1.0451 - val_gender_output_acc: 0.7099 - val_image_quality_output_acc: 0.5062 - val_age_output_acc: 0.3901 - val_weight_output_acc: 0.6380 - val_bag_output_acc: 0.5781 - val_footwear_output_acc: 0.6158 - val_pose_output_acc: 0.6003 - val_emotion_output_acc: 0.6696\n",
            "Epoch 16/50\n",
            "\n",
            "360/360 [==============================] - 128s 356ms/step - loss: 3.5419 - gender_output_loss: 0.6073 - image_quality_output_loss: 1.0901 - age_output_loss: 1.5295 - weight_output_loss: 1.0310 - bag_output_loss: 0.9625 - footwear_output_loss: 0.9219 - pose_output_loss: 0.9721 - emotion_output_loss: 1.1774 - gender_output_acc: 0.6745 - image_quality_output_acc: 0.4650 - age_output_acc: 0.3482 - weight_output_acc: 0.6024 - bag_output_acc: 0.5444 - footwear_output_acc: 0.5746 - pose_output_acc: 0.5707 - emotion_output_acc: 0.5457 - val_loss: 3.2730 - val_gender_output_loss: 0.5426 - val_image_quality_output_loss: 1.0036 - val_age_output_loss: 1.4342 - val_weight_output_loss: 0.9668 - val_bag_output_loss: 0.8832 - val_footwear_output_loss: 0.8571 - val_pose_output_loss: 0.9042 - val_emotion_output_loss: 1.0628 - val_gender_output_acc: 0.7249 - val_image_quality_output_acc: 0.5206 - val_age_output_acc: 0.3867 - val_weight_output_acc: 0.6371 - val_bag_output_acc: 0.5852 - val_footwear_output_acc: 0.6131 - val_pose_output_acc: 0.6036 - val_emotion_output_acc: 0.6531\n",
            "Epoch 16/50\n",
            "Epoch 17/50\n",
            "360/360 [==============================] - 128s 356ms/step - loss: 3.5374 - gender_output_loss: 0.6057 - image_quality_output_loss: 1.0884 - age_output_loss: 1.5258 - weight_output_loss: 1.0344 - bag_output_loss: 0.9553 - footwear_output_loss: 0.9223 - pose_output_loss: 0.9739 - emotion_output_loss: 1.1704 - gender_output_acc: 0.6821 - image_quality_output_acc: 0.4681 - age_output_acc: 0.3523 - weight_output_acc: 0.5997 - bag_output_acc: 0.5527 - footwear_output_acc: 0.5765 - pose_output_acc: 0.5793 - emotion_output_acc: 0.5536 - val_loss: 3.2542 - val_gender_output_loss: 0.5336 - val_image_quality_output_loss: 1.0033 - val_age_output_loss: 1.4266 - val_weight_output_loss: 0.9596 - val_bag_output_loss: 0.8873 - val_footwear_output_loss: 0.8531 - val_pose_output_loss: 0.9018 - val_emotion_output_loss: 1.0486 - val_gender_output_acc: 0.7283 - val_image_quality_output_acc: 0.5176 - val_age_output_acc: 0.3942 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.5768 - val_footwear_output_acc: 0.6106 - val_pose_output_acc: 0.6069 - val_emotion_output_acc: 0.6643\n",
            "Epoch 18/50\n",
            "360/360 [==============================] - 128s 354ms/step - loss: 3.5242 - gender_output_loss: 0.6079 - image_quality_output_loss: 1.0859 - age_output_loss: 1.5162 - weight_output_loss: 1.0236 - bag_output_loss: 0.9549 - footwear_output_loss: 0.9191 - pose_output_loss: 0.9705 - emotion_output_loss: 1.1684 - gender_output_acc: 0.6792 - image_quality_output_acc: 0.4761 - age_output_acc: 0.3597 - weight_output_acc: 0.6089 - bag_output_acc: 0.5529 - footwear_output_acc: 0.5808 - pose_output_acc: 0.5829 - emotion_output_acc: 0.5556 - val_loss: 3.2446 - val_gender_output_loss: 0.5322 - val_image_quality_output_loss: 1.0034 - val_age_output_loss: 1.4255 - val_weight_output_loss: 0.9601 - val_bag_output_loss: 0.8808 - val_footwear_output_loss: 0.8470 - val_pose_output_loss: 0.8990 - val_emotion_output_loss: 1.0435 - val_gender_output_acc: 0.7333 - val_image_quality_output_acc: 0.5141 - val_age_output_acc: 0.3966 - val_weight_output_acc: 0.6343 - val_bag_output_acc: 0.5871 - val_footwear_output_acc: 0.6188 - val_pose_output_acc: 0.6098 - val_emotion_output_acc: 0.6709\n",
            "Epoch 19/50\n",
            "360/360 [==============================] - 128s 355ms/step - loss: 3.5112 - gender_output_loss: 0.5977 - image_quality_output_loss: 1.0853 - age_output_loss: 1.5202 - weight_output_loss: 1.0233 - bag_output_loss: 0.9485 - footwear_output_loss: 0.9225 - pose_output_loss: 0.9582 - emotion_output_loss: 1.1658 - gender_output_acc: 0.6861 - image_quality_output_acc: 0.4628 - age_output_acc: 0.3594 - weight_output_acc: 0.6079 - bag_output_acc: 0.5556 - footwear_output_acc: 0.5727 - pose_output_acc: 0.5817 - emotion_output_acc: 0.5574 - val_loss: 3.2485 - val_gender_output_loss: 0.5430 - val_image_quality_output_loss: 1.0027 - val_age_output_loss: 1.4241 - val_weight_output_loss: 0.9539 - val_bag_output_loss: 0.8782 - val_footwear_output_loss: 0.8455 - val_pose_output_loss: 0.9013 - val_emotion_output_loss: 1.0510 - val_gender_output_acc: 0.7241 - val_image_quality_output_acc: 0.5180 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6378 - val_bag_output_acc: 0.5862 - val_footwear_output_acc: 0.6194 - val_pose_output_acc: 0.6063 - val_emotion_output_acc: 0.6627\n",
            "Epoch 20/50\n",
            "360/360 [==============================] - 128s 356ms/step - loss: 3.5029 - gender_output_loss: 0.5908 - image_quality_output_loss: 1.0772 - age_output_loss: 1.5237 - weight_output_loss: 1.0266 - bag_output_loss: 0.9432 - footwear_output_loss: 0.9181 - pose_output_loss: 0.9618 - emotion_output_loss: 1.1664 - gender_output_acc: 0.6931 - image_quality_output_acc: 0.4796 - age_output_acc: 0.3590 - weight_output_acc: 0.6015 - bag_output_acc: 0.5622 - footwear_output_acc: 0.5801 - pose_output_acc: 0.5819 - emotion_output_acc: 0.5627 - val_loss: 3.2449 - val_gender_output_loss: 0.5388 - val_image_quality_output_loss: 1.0001 - val_age_output_loss: 1.4222 - val_weight_output_loss: 0.9555 - val_bag_output_loss: 0.8769 - val_footwear_output_loss: 0.8502 - val_pose_output_loss: 0.8981 - val_emotion_output_loss: 1.0428 - val_gender_output_acc: 0.7240 - val_image_quality_output_acc: 0.5184 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6375 - val_bag_output_acc: 0.5859 - val_footwear_output_acc: 0.6152 - val_pose_output_acc: 0.6078 - val_emotion_output_acc: 0.6681\n",
            "Epoch 21/50\n",
            "360/360 [==============================] - 128s 356ms/step - loss: 3.5050 - gender_output_loss: 0.5990 - image_quality_output_loss: 1.0729 - age_output_loss: 1.5198 - weight_output_loss: 1.0232 - bag_output_loss: 0.9447 - footwear_output_loss: 0.9153 - pose_output_loss: 0.9644 - emotion_output_loss: 1.1639 - gender_output_acc: 0.6843 - image_quality_output_acc: 0.4839 - age_output_acc: 0.3533 - weight_output_acc: 0.6040 - bag_output_acc: 0.5623 - footwear_output_acc: 0.5795 - pose_output_acc: 0.5843 - emotion_output_acc: 0.5607 - val_loss: 3.2293 - val_gender_output_loss: 0.5279 - val_image_quality_output_loss: 1.0003 - val_age_output_loss: 1.4236 - val_weight_output_loss: 0.9531 - val_bag_output_loss: 0.8751 - val_footwear_output_loss: 0.8451 - val_pose_output_loss: 0.8957 - val_emotion_output_loss: 1.0416 - val_gender_output_acc: 0.7357 - val_image_quality_output_acc: 0.5217 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6385 - val_bag_output_acc: 0.5907 - val_footwear_output_acc: 0.6183 - val_pose_output_acc: 0.6099 - val_emotion_output_acc: 0.6696\n",
            "Epoch 22/50\n",
            "360/360 [==============================] - 128s 355ms/step - loss: 3.5080 - gender_output_loss: 0.5970 - image_quality_output_loss: 1.0822 - age_output_loss: 1.5176 - weight_output_loss: 1.0292 - bag_output_loss: 0.9444 - footwear_output_loss: 0.9156 - pose_output_loss: 0.9633 - emotion_output_loss: 1.1625 - gender_output_acc: 0.6816 - image_quality_output_acc: 0.4729 - age_output_acc: 0.3619 - weight_output_acc: 0.6062 - bag_output_acc: 0.5600 - footwear_output_acc: 0.5786 - pose_output_acc: 0.5819 - emotion_output_acc: 0.5606 - val_loss: 3.2288 - val_gender_output_loss: 0.5306 - val_image_quality_output_loss: 1.0006 - val_age_output_loss: 1.4197 - val_weight_output_loss: 0.9541 - val_bag_output_loss: 0.8740 - val_footwear_output_loss: 0.8426 - val_pose_output_loss: 0.8955 - val_emotion_output_loss: 1.0424 - val_gender_output_acc: 0.7303 - val_image_quality_output_acc: 0.5155 - val_age_output_acc: 0.3997 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5918 - val_footwear_output_acc: 0.6179 - val_pose_output_acc: 0.6100 - val_emotion_output_acc: 0.6688\n",
            "Epoch 23/50\n",
            "360/360 [==============================] - 127s 354ms/step - loss: 3.4974 - gender_output_loss: 0.5934 - image_quality_output_loss: 1.0817 - age_output_loss: 1.5111 - weight_output_loss: 1.0226 - bag_output_loss: 0.9483 - footwear_output_loss: 0.9136 - pose_output_loss: 0.9611 - emotion_output_loss: 1.1604 - gender_output_acc: 0.6959 - image_quality_output_acc: 0.4753 - age_output_acc: 0.3632 - weight_output_acc: 0.6017 - bag_output_acc: 0.5563 - footwear_output_acc: 0.5786 - pose_output_acc: 0.5847 - emotion_output_acc: 0.5616 - val_loss: 3.2226 - val_gender_output_loss: 0.5257 - val_image_quality_output_loss: 1.0002 - val_age_output_loss: 1.4201 - val_weight_output_loss: 0.9512 - val_bag_output_loss: 0.8738 - val_footwear_output_loss: 0.8431 - val_pose_output_loss: 0.8956 - val_emotion_output_loss: 1.0416 - val_gender_output_acc: 0.7361 - val_image_quality_output_acc: 0.5177 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6388 - val_bag_output_acc: 0.5910 - val_footwear_output_acc: 0.6208 - val_pose_output_acc: 0.6065 - val_emotion_output_acc: 0.6684\n",
            "Epoch 24/50\n",
            "360/360 [==============================] - 127s 354ms/step - loss: 3.4858 - gender_output_loss: 0.5888 - image_quality_output_loss: 1.0811 - age_output_loss: 1.5107 - weight_output_loss: 1.0178 - bag_output_loss: 0.9428 - footwear_output_loss: 0.9120 - pose_output_loss: 0.9617 - emotion_output_loss: 1.1567 - gender_output_acc: 0.6903 - image_quality_output_acc: 0.4790 - age_output_acc: 0.3585 - weight_output_acc: 0.6059 - bag_output_acc: 0.5593 - footwear_output_acc: 0.5799 - pose_output_acc: 0.5784 - emotion_output_acc: 0.5619 - val_loss: 3.2223 - val_gender_output_loss: 0.5267 - val_image_quality_output_loss: 1.0000 - val_age_output_loss: 1.4179 - val_weight_output_loss: 0.9511 - val_bag_output_loss: 0.8730 - val_footwear_output_loss: 0.8432 - val_pose_output_loss: 0.8921 - val_emotion_output_loss: 1.0530 - val_gender_output_acc: 0.7372 - val_image_quality_output_acc: 0.5159 - val_age_output_acc: 0.4000 - val_weight_output_acc: 0.6392 - val_bag_output_acc: 0.5936 - val_footwear_output_acc: 0.6191 - val_pose_output_acc: 0.6113 - val_emotion_output_acc: 0.6615\n",
            "Epoch 25/50\n",
            "Epoch 24/50\n",
            "360/360 [==============================] - 128s 355ms/step - loss: 3.4976 - gender_output_loss: 0.5928 - image_quality_output_loss: 1.0817 - age_output_loss: 1.5178 - weight_output_loss: 1.0218 - bag_output_loss: 0.9430 - footwear_output_loss: 0.9144 - pose_output_loss: 0.9665 - emotion_output_loss: 1.1525 - gender_output_acc: 0.6951 - image_quality_output_acc: 0.4739 - age_output_acc: 0.3595 - weight_output_acc: 0.6064 - bag_output_acc: 0.5604 - footwear_output_acc: 0.5772 - pose_output_acc: 0.5809 - emotion_output_acc: 0.5639 - val_loss: 3.2167 - val_gender_output_loss: 0.5249 - val_image_quality_output_loss: 0.9992 - val_age_output_loss: 1.4193 - val_weight_output_loss: 0.9487 - val_bag_output_loss: 0.8720 - val_footwear_output_loss: 0.8407 - val_pose_output_loss: 0.8926 - val_emotion_output_loss: 1.0443 - val_gender_output_acc: 0.7378 - val_image_quality_output_acc: 0.5161 - val_age_output_acc: 0.3977 - val_weight_output_acc: 0.6387 - val_bag_output_acc: 0.5938 - val_footwear_output_acc: 0.6210 - val_pose_output_acc: 0.6104 - val_emotion_output_acc: 0.6668\n",
            "Epoch 26/50\n",
            "360/360 [==============================] - 128s 355ms/step - loss: 3.4856 - gender_output_loss: 0.5940 - image_quality_output_loss: 1.0856 - age_output_loss: 1.5154 - weight_output_loss: 1.0139 - bag_output_loss: 0.9352 - footwear_output_loss: 0.9116 - pose_output_loss: 0.9585 - emotion_output_loss: 1.1587 - gender_output_acc: 0.6895 - image_quality_output_acc: 0.4773 - age_output_acc: 0.3577 - weight_output_acc: 0.6105 - bag_output_acc: 0.5673 - footwear_output_acc: 0.5832 - pose_output_acc: 0.5821 - emotion_output_acc: 0.5622 - val_loss: 3.2114 - val_gender_output_loss: 0.5200 - val_image_quality_output_loss: 1.0002 - val_age_output_loss: 1.4185 - val_weight_output_loss: 0.9489 - val_bag_output_loss: 0.8717 - val_footwear_output_loss: 0.8416 - val_pose_output_loss: 0.8912 - val_emotion_output_loss: 1.0348 - val_gender_output_acc: 0.7432 - val_image_quality_output_acc: 0.5104 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6393 - val_bag_output_acc: 0.5932 - val_footwear_output_acc: 0.6208 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.6730\n",
            "Epoch 27/50\n",
            "360/360 [==============================] - 128s 356ms/step - loss: 3.4934 - gender_output_loss: 0.5957 - image_quality_output_loss: 1.0802 - age_output_loss: 1.5131 - weight_output_loss: 1.0190 - bag_output_loss: 0.9385 - footwear_output_loss: 0.9140 - pose_output_loss: 0.9640 - emotion_output_loss: 1.1577 - gender_output_acc: 0.6908 - image_quality_output_acc: 0.4776 - age_output_acc: 0.3542 - weight_output_acc: 0.6003 - bag_output_acc: 0.5638 - footwear_output_acc: 0.5806 - pose_output_acc: 0.5813 - emotion_output_acc: 0.5638 - val_loss: 3.2142 - val_gender_output_loss: 0.5240 - val_image_quality_output_loss: 0.9989 - val_age_output_loss: 1.4173 - val_weight_output_loss: 0.9497 - val_bag_output_loss: 0.8713 - val_footwear_output_loss: 0.8403 - val_pose_output_loss: 0.8931 - val_emotion_output_loss: 1.0334 - val_gender_output_acc: 0.7390 - val_image_quality_output_acc: 0.5177 - val_age_output_acc: 0.3980 - val_weight_output_acc: 0.6392 - val_bag_output_acc: 0.5938 - val_footwear_output_acc: 0.6201 - val_pose_output_acc: 0.6091 - val_emotion_output_acc: 0.6747\n",
            "Epoch 28/50\n",
            "360/360 [==============================] - 128s 355ms/step - loss: 3.4815 - gender_output_loss: 0.5931 - image_quality_output_loss: 1.0824 - age_output_loss: 1.5092 - weight_output_loss: 1.0131 - bag_output_loss: 0.9397 - footwear_output_loss: 0.9066 - pose_output_loss: 0.9631 - emotion_output_loss: 1.1562 - gender_output_acc: 0.6901 - image_quality_output_acc: 0.4773 - age_output_acc: 0.3593 - weight_output_acc: 0.6062 - bag_output_acc: 0.5651 - footwear_output_acc: 0.5797 - pose_output_acc: 0.5834 - emotion_output_acc: 0.5635 - val_loss: 3.2110 - val_gender_output_loss: 0.5221 - val_image_quality_output_loss: 0.9984 - val_age_output_loss: 1.4167 - val_weight_output_loss: 0.9492 - val_bag_output_loss: 0.8713 - val_footwear_output_loss: 0.8399 - val_pose_output_loss: 0.8905 - val_emotion_output_loss: 1.0365 - val_gender_output_acc: 0.7398 - val_image_quality_output_acc: 0.5171 - val_age_output_acc: 0.4010 - val_weight_output_acc: 0.6395 - val_bag_output_acc: 0.5935 - val_footwear_output_acc: 0.6194 - val_pose_output_acc: 0.6121 - val_emotion_output_acc: 0.6719\n",
            "Epoch 29/50\n",
            "360/360 [==============================] - 128s 355ms/step - loss: 3.4728 - gender_output_loss: 0.5817 - image_quality_output_loss: 1.0832 - age_output_loss: 1.5163 - weight_output_loss: 1.0099 - bag_output_loss: 0.9395 - footwear_output_loss: 0.9090 - pose_output_loss: 0.9620 - emotion_output_loss: 1.1545 - gender_output_acc: 0.7021 - image_quality_output_acc: 0.4718 - age_output_acc: 0.3544 - weight_output_acc: 0.6123 - bag_output_acc: 0.5616 - footwear_output_acc: 0.5847 - pose_output_acc: 0.5841 - emotion_output_acc: 0.5701 - val_loss: 3.2125 - val_gender_output_loss: 0.5242 - val_image_quality_output_loss: 0.9986 - val_age_output_loss: 1.4167 - val_weight_output_loss: 0.9478 - val_bag_output_loss: 0.8706 - val_footwear_output_loss: 0.8398 - val_pose_output_loss: 0.8910 - val_emotion_output_loss: 1.0424 - val_gender_output_acc: 0.7364 - val_image_quality_output_acc: 0.5169 - val_age_output_acc: 0.3990 - val_weight_output_acc: 0.6385 - val_bag_output_acc: 0.5934 - val_footwear_output_acc: 0.6200 - val_pose_output_acc: 0.6098 - val_emotion_output_acc: 0.6687\n",
            "Epoch 30/50\n",
            "360/360 [==============================] - 128s 356ms/step - loss: 3.4868 - gender_output_loss: 0.5887 - image_quality_output_loss: 1.0810 - age_output_loss: 1.5146 - weight_output_loss: 1.0162 - bag_output_loss: 0.9399 - footwear_output_loss: 0.9144 - pose_output_loss: 0.9651 - emotion_output_loss: 1.1544 - gender_output_acc: 0.6934 - image_quality_output_acc: 0.4741 - age_output_acc: 0.3609 - weight_output_acc: 0.6060 - bag_output_acc: 0.5568 - footwear_output_acc: 0.5758 - pose_output_acc: 0.5824 - emotion_output_acc: 0.5617 - val_loss: 3.2107 - val_gender_output_loss: 0.5229 - val_image_quality_output_loss: 0.9986 - val_age_output_loss: 1.4166 - val_weight_output_loss: 0.9491 - val_bag_output_loss: 0.8706 - val_footwear_output_loss: 0.8390 - val_pose_output_loss: 0.8910 - val_emotion_output_loss: 1.0335 - val_gender_output_acc: 0.7378 - val_image_quality_output_acc: 0.5187 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5937 - val_footwear_output_acc: 0.6216 - val_pose_output_acc: 0.6113 - val_emotion_output_acc: 0.6743\n",
            "Epoch 31/50\n",
            "360/360 [==============================] - 128s 355ms/step - loss: 3.4746 - gender_output_loss: 0.5824 - image_quality_output_loss: 1.0791 - age_output_loss: 1.5095 - weight_output_loss: 1.0178 - bag_output_loss: 0.9433 - footwear_output_loss: 0.9118 - pose_output_loss: 0.9458 - emotion_output_loss: 1.1596 - gender_output_acc: 0.6958 - image_quality_output_acc: 0.4752 - age_output_acc: 0.3620 - weight_output_acc: 0.6042 - bag_output_acc: 0.5568 - footwear_output_acc: 0.5845 - pose_output_acc: 0.5829 - emotion_output_acc: 0.5694 - val_loss: 3.2077 - val_gender_output_loss: 0.5204 - val_image_quality_output_loss: 0.9978 - val_age_output_loss: 1.4169 - val_weight_output_loss: 0.9488 - val_bag_output_loss: 0.8699 - val_footwear_output_loss: 0.8385 - val_pose_output_loss: 0.8904 - val_emotion_output_loss: 1.0371 - val_gender_output_acc: 0.7427 - val_image_quality_output_acc: 0.5186 - val_age_output_acc: 0.3982 - val_weight_output_acc: 0.6398 - val_bag_output_acc: 0.5944 - val_footwear_output_acc: 0.6213 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.6715\n",
            "Epoch 32/50\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 3.4795 - gender_output_loss: 0.5966 - image_quality_output_loss: 1.0776 - age_output_loss: 1.5047 - weight_output_loss: 1.0111 - bag_output_loss: 0.9394 - footwear_output_loss: 0.9080 - pose_output_loss: 0.9569 - emotion_output_loss: 1.1584 - gender_output_acc: 0.6909 - image_quality_output_acc: 0.4757 - age_output_acc: 0.3617 - weight_output_acc: 0.6091 - bag_output_acc: 0.5628 - footwear_output_acc: 0.5844 - pose_output_acc: 0.5828 - emotion_output_acc: 0.5616 - val_loss: 3.2078 - val_gender_output_loss: 0.5213 - val_image_quality_output_loss: 0.9981 - val_age_output_loss: 1.4154 - val_weight_output_loss: 0.9472 - val_bag_output_loss: 0.8698 - val_footwear_output_loss: 0.8390 - val_pose_output_loss: 0.8910 - val_emotion_output_loss: 1.0421 - val_gender_output_acc: 0.7406 - val_image_quality_output_acc: 0.5171 - val_age_output_acc: 0.4004 - val_weight_output_acc: 0.6395 - val_bag_output_acc: 0.5943 - val_footwear_output_acc: 0.6206 - val_pose_output_acc: 0.6123 - val_emotion_output_acc: 0.6686\n",
            "Epoch 33/50\n",
            "360/360 [==============================] - 127s 354ms/step - loss: 3.4987 - gender_output_loss: 0.5958 - image_quality_output_loss: 1.0846 - age_output_loss: 1.5119 - weight_output_loss: 1.0180 - bag_output_loss: 0.9430 - footwear_output_loss: 0.9179 - pose_output_loss: 0.9686 - emotion_output_loss: 1.1535 - gender_output_acc: 0.6862 - image_quality_output_acc: 0.4773 - age_output_acc: 0.3621 - weight_output_acc: 0.6096 - bag_output_acc: 0.5574 - footwear_output_acc: 0.5749 - pose_output_acc: 0.5790 - emotion_output_acc: 0.5628 - val_loss: 3.2067 - val_gender_output_loss: 0.5208 - val_image_quality_output_loss: 0.9983 - val_age_output_loss: 1.4158 - val_weight_output_loss: 0.9476 - val_bag_output_loss: 0.8694 - val_footwear_output_loss: 0.8388 - val_pose_output_loss: 0.8902 - val_emotion_output_loss: 1.0366 - val_gender_output_acc: 0.7417 - val_image_quality_output_acc: 0.5179 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6394 - val_bag_output_acc: 0.5944 - val_footwear_output_acc: 0.6207 - val_pose_output_acc: 0.6105 - val_emotion_output_acc: 0.6717\n",
            "Epoch 34/50\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 3.4805 - gender_output_loss: 0.5826 - image_quality_output_loss: 1.0737 - age_output_loss: 1.5188 - weight_output_loss: 1.0206 - bag_output_loss: 0.9361 - footwear_output_loss: 0.9141 - pose_output_loss: 0.9569 - emotion_output_loss: 1.1594 - gender_output_acc: 0.6944 - image_quality_output_acc: 0.4732 - age_output_acc: 0.3588 - weight_output_acc: 0.6083 - bag_output_acc: 0.5685 - footwear_output_acc: 0.5836 - pose_output_acc: 0.5855 - emotion_output_acc: 0.5680 - val_loss: 3.2070 - val_gender_output_loss: 0.5215 - val_image_quality_output_loss: 0.9990 - val_age_output_loss: 1.4159 - val_weight_output_loss: 0.9472 - val_bag_output_loss: 0.8692 - val_footwear_output_loss: 0.8385 - val_pose_output_loss: 0.8904 - val_emotion_output_loss: 1.0373 - val_gender_output_acc: 0.7408 - val_image_quality_output_acc: 0.5148 - val_age_output_acc: 0.3995 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5944 - val_footwear_output_acc: 0.6206 - val_pose_output_acc: 0.6111 - val_emotion_output_acc: 0.6707\n",
            "Epoch 35/50\n",
            "Epoch 34/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 3.4697 - gender_output_loss: 0.5886 - image_quality_output_loss: 1.0777 - age_output_loss: 1.5151 - weight_output_loss: 1.0124 - bag_output_loss: 0.9348 - footwear_output_loss: 0.9014 - pose_output_loss: 0.9526 - emotion_output_loss: 1.1566 - gender_output_acc: 0.6872 - image_quality_output_acc: 0.4786 - age_output_acc: 0.3605 - weight_output_acc: 0.6059 - bag_output_acc: 0.5675 - footwear_output_acc: 0.5875 - pose_output_acc: 0.5905 - emotion_output_acc: 0.5648Epoch 35/50\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 3.4698 - gender_output_loss: 0.5884 - image_quality_output_loss: 1.0779 - age_output_loss: 1.5145 - weight_output_loss: 1.0124 - bag_output_loss: 0.9350 - footwear_output_loss: 0.9019 - pose_output_loss: 0.9527 - emotion_output_loss: 1.1574 - gender_output_acc: 0.6873 - image_quality_output_acc: 0.4784 - age_output_acc: 0.3606 - weight_output_acc: 0.6059 - bag_output_acc: 0.5674 - footwear_output_acc: 0.5869 - pose_output_acc: 0.5906 - emotion_output_acc: 0.5646 - val_loss: 3.2059 - val_gender_output_loss: 0.5206 - val_image_quality_output_loss: 0.9986 - val_age_output_loss: 1.4155 - val_weight_output_loss: 0.9474 - val_bag_output_loss: 0.8692 - val_footwear_output_loss: 0.8383 - val_pose_output_loss: 0.8901 - val_emotion_output_loss: 1.0370 - val_gender_output_acc: 0.7414 - val_image_quality_output_acc: 0.5162 - val_age_output_acc: 0.3994 - val_weight_output_acc: 0.6389 - val_bag_output_acc: 0.5954 - val_footwear_output_acc: 0.6212 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.6717\n",
            "Epoch 36/50\n",
            "360/360 [==============================] - 127s 352ms/step - loss: 3.4784 - gender_output_loss: 0.5870 - image_quality_output_loss: 1.0771 - age_output_loss: 1.5096 - weight_output_loss: 1.0156 - bag_output_loss: 0.9395 - footwear_output_loss: 0.9129 - pose_output_loss: 0.9557 - emotion_output_loss: 1.1567 - gender_output_acc: 0.6951 - image_quality_output_acc: 0.4699 - age_output_acc: 0.3660 - weight_output_acc: 0.6095 - bag_output_acc: 0.5617 - footwear_output_acc: 0.5809 - pose_output_acc: 0.5901 - emotion_output_acc: 0.5621 - val_loss: 3.2036 - val_gender_output_loss: 0.5193 - val_image_quality_output_loss: 0.9975 - val_age_output_loss: 1.4152 - val_weight_output_loss: 0.9468 - val_bag_output_loss: 0.8692 - val_footwear_output_loss: 0.8381 - val_pose_output_loss: 0.8901 - val_emotion_output_loss: 1.0340 - val_gender_output_acc: 0.7429 - val_image_quality_output_acc: 0.5183 - val_age_output_acc: 0.3999 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5952 - val_footwear_output_acc: 0.6207 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.6733\n",
            "Epoch 37/50\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 3.4698 - gender_output_loss: 0.5884 - image_quality_output_loss: 1.0779 - age_output_loss: 1.5145 - weight_output_loss: 1.0124 - bag_output_loss: 0.9350 - footwear_output_loss: 0.9019 - pose_output_loss: 0.9527 - emotion_output_loss: 1.1574 - gender_output_acc: 0.6873 - image_quality_output_acc: 0.4784 - age_output_acc: 0.3606 - weight_output_acc: 0.6059 - bag_output_acc: 0.5674 - footwear_output_acc: 0.5869 - pose_output_acc: 0.5906 - emotion_output_acc: 0.5646 - val_loss: 3.2059 - val_gender_output_loss: 0.5206 - val_image_quality_output_loss: 0.9986 - val_age_output_loss: 1.4155 - val_weight_output_loss: 0.9474 - val_bag_output_loss: 0.8692 - val_footwear_output_loss: 0.8383 - val_pose_output_loss: 0.8901 - val_emotion_output_loss: 1.0370 - val_gender_output_acc: 0.7414 - val_image_quality_output_acc: 0.5162 - val_age_output_acc: 0.3994 - val_weight_output_acc: 0.6389 - val_bag_output_acc: 0.5954 - val_footwear_output_acc: 0.6212 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.6717\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 3.4807 - gender_output_loss: 0.5906 - image_quality_output_loss: 1.0851 - age_output_loss: 1.5131 - weight_output_loss: 1.0131 - bag_output_loss: 0.9342 - footwear_output_loss: 0.9104 - pose_output_loss: 0.9623 - emotion_output_loss: 1.1562 - gender_output_acc: 0.6933 - image_quality_output_acc: 0.4688 - age_output_acc: 0.3588 - weight_output_acc: 0.6095 - bag_output_acc: 0.5636 - footwear_output_acc: 0.5843 - pose_output_acc: 0.5799 - emotion_output_acc: 0.5641 - val_loss: 3.2051 - val_gender_output_loss: 0.5202 - val_image_quality_output_loss: 0.9978 - val_age_output_loss: 1.4152 - val_weight_output_loss: 0.9477 - val_bag_output_loss: 0.8687 - val_footwear_output_loss: 0.8381 - val_pose_output_loss: 0.8898 - val_emotion_output_loss: 1.0370 - val_gender_output_acc: 0.7417 - val_image_quality_output_acc: 0.5173 - val_age_output_acc: 0.3999 - val_weight_output_acc: 0.6397 - val_bag_output_acc: 0.5949 - val_footwear_output_acc: 0.6219 - val_pose_output_acc: 0.6117 - val_emotion_output_acc: 0.6718\n",
            "Epoch 38/50\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 3.4810 - gender_output_loss: 0.5808 - image_quality_output_loss: 1.0816 - age_output_loss: 1.5188 - weight_output_loss: 1.0191 - bag_output_loss: 0.9329 - footwear_output_loss: 0.9180 - pose_output_loss: 0.9586 - emotion_output_loss: 1.1625 - gender_output_acc: 0.7013 - image_quality_output_acc: 0.4692 - age_output_acc: 0.3553 - weight_output_acc: 0.6077 - bag_output_acc: 0.5616 - footwear_output_acc: 0.5806 - pose_output_acc: 0.5820 - emotion_output_acc: 0.5657 - val_loss: 3.2023 - val_gender_output_loss: 0.5186 - val_image_quality_output_loss: 0.9979 - val_age_output_loss: 1.4150 - val_weight_output_loss: 0.9466 - val_bag_output_loss: 0.8685 - val_footwear_output_loss: 0.8380 - val_pose_output_loss: 0.8888 - val_emotion_output_loss: 1.0366 - val_gender_output_acc: 0.7431 - val_image_quality_output_acc: 0.5161 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6393 - val_bag_output_acc: 0.5958 - val_footwear_output_acc: 0.6222 - val_pose_output_acc: 0.6127 - val_emotion_output_acc: 0.6714\n",
            "Epoch 39/50\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 3.4844 - gender_output_loss: 0.5928 - image_quality_output_loss: 1.0751 - age_output_loss: 1.5028 - weight_output_loss: 1.0177 - bag_output_loss: 0.9445 - footwear_output_loss: 0.9087 - pose_output_loss: 0.9628 - emotion_output_loss: 1.1605 - gender_output_acc: 0.6896 - image_quality_output_acc: 0.4776 - age_output_acc: 0.3606 - weight_output_acc: 0.6122 - bag_output_acc: 0.5589 - footwear_output_acc: 0.5822 - pose_output_acc: 0.5771 - emotion_output_acc: 0.5619 - val_loss: 3.2039 - val_gender_output_loss: 0.5197 - val_image_quality_output_loss: 0.9976 - val_age_output_loss: 1.4155 - val_weight_output_loss: 0.9465 - val_bag_output_loss: 0.8686 - val_footwear_output_loss: 0.8383 - val_pose_output_loss: 0.8902 - val_emotion_output_loss: 1.0359 - val_gender_output_acc: 0.7418 - val_image_quality_output_acc: 0.5172 - val_age_output_acc: 0.4005 - val_weight_output_acc: 0.6387 - val_bag_output_acc: 0.5958 - val_footwear_output_acc: 0.6212 - val_pose_output_acc: 0.6103 - val_emotion_output_acc: 0.6720\n",
            "Epoch 40/50\n",
            "360/360 [==============================] - 127s 352ms/step - loss: 3.4848 - gender_output_loss: 0.5932 - image_quality_output_loss: 1.0802 - age_output_loss: 1.5084 - weight_output_loss: 1.0208 - bag_output_loss: 0.9361 - footwear_output_loss: 0.9088 - pose_output_loss: 0.9595 - emotion_output_loss: 1.1568 - gender_output_acc: 0.6936 - image_quality_output_acc: 0.4747 - age_output_acc: 0.3619 - weight_output_acc: 0.6085 - bag_output_acc: 0.5617 - footwear_output_acc: 0.5813 - pose_output_acc: 0.5831 - emotion_output_acc: 0.5682 - val_loss: 3.2061 - val_gender_output_loss: 0.5216 - val_image_quality_output_loss: 0.9985 - val_age_output_loss: 1.4156 - val_weight_output_loss: 0.9475 - val_bag_output_loss: 0.8686 - val_footwear_output_loss: 0.8376 - val_pose_output_loss: 0.8897 - val_emotion_output_loss: 1.0372 - val_gender_output_acc: 0.7396 - val_image_quality_output_acc: 0.5143 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6393 - val_bag_output_acc: 0.5959 - val_footwear_output_acc: 0.6220 - val_pose_output_acc: 0.6102 - val_emotion_output_acc: 0.6722\n",
            "Epoch 41/50\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 3.4764 - gender_output_loss: 0.5840 - image_quality_output_loss: 1.0778 - age_output_loss: 1.5125 - weight_output_loss: 1.0203 - bag_output_loss: 0.9384 - footwear_output_loss: 0.9062 - pose_output_loss: 0.9568 - emotion_output_loss: 1.1604 - gender_output_acc: 0.6924 - image_quality_output_acc: 0.4742 - age_output_acc: 0.3542 - weight_output_acc: 0.6051 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5771 - pose_output_acc: 0.5842 - emotion_output_acc: 0.5635 - val_loss: 3.2040 - val_gender_output_loss: 0.5203 - val_image_quality_output_loss: 0.9976 - val_age_output_loss: 1.4150 - val_weight_output_loss: 0.9467 - val_bag_output_loss: 0.8688 - val_footwear_output_loss: 0.8379 - val_pose_output_loss: 0.8890 - val_emotion_output_loss: 1.0363 - val_gender_output_acc: 0.7419 - val_image_quality_output_acc: 0.5169 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6395 - val_bag_output_acc: 0.5945 - val_footwear_output_acc: 0.6215 - val_pose_output_acc: 0.6119 - val_emotion_output_acc: 0.6727\n",
            "Epoch 42/50\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 3.4803 - gender_output_loss: 0.5860 - image_quality_output_loss: 1.0726 - age_output_loss: 1.5141 - weight_output_loss: 1.0225 - bag_output_loss: 0.9383 - footwear_output_loss: 0.9082 - pose_output_loss: 0.9569 - emotion_output_loss: 1.1585 - gender_output_acc: 0.6949 - image_quality_output_acc: 0.4839 - age_output_acc: 0.3588 - weight_output_acc: 0.6037 - bag_output_acc: 0.5655 - footwear_output_acc: 0.5859 - pose_output_acc: 0.5871 - emotion_output_acc: 0.5623 - val_loss: 3.2028 - val_gender_output_loss: 0.5191 - val_image_quality_output_loss: 0.9981 - val_age_output_loss: 1.4157 - val_weight_output_loss: 0.9467 - val_bag_output_loss: 0.8684 - val_footwear_output_loss: 0.8374 - val_pose_output_loss: 0.8888 - val_emotion_output_loss: 1.0377 - val_gender_output_acc: 0.7422 - val_image_quality_output_acc: 0.5156 - val_age_output_acc: 0.3997 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5965 - val_footwear_output_acc: 0.6227 - val_pose_output_acc: 0.6118 - val_emotion_output_acc: 0.6716\n",
            "Epoch 43/50\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 3.4775 - gender_output_loss: 0.5911 - image_quality_output_loss: 1.0772 - age_output_loss: 1.5071 - weight_output_loss: 1.0147 - bag_output_loss: 0.9412 - footwear_output_loss: 0.9059 - pose_output_loss: 0.9570 - emotion_output_loss: 1.1570 - gender_output_acc: 0.6940 - image_quality_output_acc: 0.4788 - age_output_acc: 0.3623 - weight_output_acc: 0.6122 - bag_output_acc: 0.5587 - footwear_output_acc: 0.5885 - pose_output_acc: 0.5901 - emotion_output_acc: 0.5617 - val_loss: 3.2028 - val_gender_output_loss: 0.5192 - val_image_quality_output_loss: 0.9983 - val_age_output_loss: 1.4146 - val_weight_output_loss: 0.9473 - val_bag_output_loss: 0.8683 - val_footwear_output_loss: 0.8375 - val_pose_output_loss: 0.8889 - val_emotion_output_loss: 1.0370 - val_gender_output_acc: 0.7419 - val_image_quality_output_acc: 0.5148 - val_age_output_acc: 0.4010 - val_weight_output_acc: 0.6393 - val_bag_output_acc: 0.5956 - val_footwear_output_acc: 0.6223 - val_pose_output_acc: 0.6118 - val_emotion_output_acc: 0.6719\n",
            "Epoch 44/50\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 3.4879 - gender_output_loss: 0.5924 - image_quality_output_loss: 1.0809 - age_output_loss: 1.5095 - weight_output_loss: 1.0167 - bag_output_loss: 0.9434 - footwear_output_loss: 0.9109 - pose_output_loss: 0.9651 - emotion_output_loss: 1.1535 - gender_output_acc: 0.6883 - image_quality_output_acc: 0.4812 - age_output_acc: 0.3605 - weight_output_acc: 0.6122 - bag_output_acc: 0.5609 - footwear_output_acc: 0.5839 - pose_output_acc: 0.5813 - emotion_output_acc: 0.5680 - val_loss: 3.2040 - val_gender_output_loss: 0.5201 - val_image_quality_output_loss: 0.9982 - val_age_output_loss: 1.4151 - val_weight_output_loss: 0.9468 - val_bag_output_loss: 0.8686 - val_footwear_output_loss: 0.8379 - val_pose_output_loss: 0.8894 - val_emotion_output_loss: 1.0363 - val_gender_output_acc: 0.7406 - val_image_quality_output_acc: 0.5146 - val_age_output_acc: 0.3997 - val_weight_output_acc: 0.6397 - val_bag_output_acc: 0.5953 - val_footwear_output_acc: 0.6227 - val_pose_output_acc: 0.6114 - val_emotion_output_acc: 0.6719\n",
            "Epoch 45/50\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 3.4733 - gender_output_loss: 0.5791 - image_quality_output_loss: 1.0778 - age_output_loss: 1.5097 - weight_output_loss: 1.0180 - bag_output_loss: 0.9390 - footwear_output_loss: 0.9117 - pose_output_loss: 0.9612 - emotion_output_loss: 1.1524 - gender_output_acc: 0.7038 - image_quality_output_acc: 0.4830 - age_output_acc: 0.3625 - weight_output_acc: 0.6084 - bag_output_acc: 0.5602 - footwear_output_acc: 0.5845 - pose_output_acc: 0.5801 - emotion_output_acc: 0.5648 - val_loss: 3.2027 - val_gender_output_loss: 0.5190 - val_image_quality_output_loss: 0.9983 - val_age_output_loss: 1.4144 - val_weight_output_loss: 0.9471 - val_bag_output_loss: 0.8683 - val_footwear_output_loss: 0.8381 - val_pose_output_loss: 0.8891 - val_emotion_output_loss: 1.0350 - val_gender_output_acc: 0.7426 - val_image_quality_output_acc: 0.5178 - val_age_output_acc: 0.4010 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5970 - val_footwear_output_acc: 0.6206 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.6727\n",
            "Epoch 46/50\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 3.4721 - gender_output_loss: 0.5803 - image_quality_output_loss: 1.0787 - age_output_loss: 1.5104 - weight_output_loss: 1.0195 - bag_output_loss: 0.9318 - footwear_output_loss: 0.9139 - pose_output_loss: 0.9553 - emotion_output_loss: 1.1539 - gender_output_acc: 0.7006 - image_quality_output_acc: 0.4778 - age_output_acc: 0.3605 - weight_output_acc: 0.6078 - bag_output_acc: 0.5692 - footwear_output_acc: 0.5812 - pose_output_acc: 0.5846 - emotion_output_acc: 0.5669 - val_loss: 3.2023 - val_gender_output_loss: 0.5187 - val_image_quality_output_loss: 0.9977 - val_age_output_loss: 1.4153 - val_weight_output_loss: 0.9467 - val_bag_output_loss: 0.8687 - val_footwear_output_loss: 0.8371 - val_pose_output_loss: 0.8892 - val_emotion_output_loss: 1.0375 - val_gender_output_acc: 0.7436 - val_image_quality_output_acc: 0.5180 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6392 - val_bag_output_acc: 0.5951 - val_footwear_output_acc: 0.6226 - val_pose_output_acc: 0.6122 - val_emotion_output_acc: 0.6714\n",
            "Epoch 47/50\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 3.4675 - gender_output_loss: 0.5804 - image_quality_output_loss: 1.0777 - age_output_loss: 1.5084 - weight_output_loss: 1.0103 - bag_output_loss: 0.9357 - footwear_output_loss: 0.9125 - pose_output_loss: 0.9601 - emotion_output_loss: 1.1570 - gender_output_acc: 0.6984 - image_quality_output_acc: 0.4792 - age_output_acc: 0.3634 - weight_output_acc: 0.6109 - bag_output_acc: 0.5642 - footwear_output_acc: 0.5833 - pose_output_acc: 0.5851 - emotion_output_acc: 0.5607 - val_loss: 3.2029 - val_gender_output_loss: 0.5195 - val_image_quality_output_loss: 0.9982 - val_age_output_loss: 1.4148 - val_weight_output_loss: 0.9469 - val_bag_output_loss: 0.8683 - val_footwear_output_loss: 0.8374 - val_pose_output_loss: 0.8885 - val_emotion_output_loss: 1.0379 - val_gender_output_acc: 0.7421 - val_image_quality_output_acc: 0.5171 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6393 - val_bag_output_acc: 0.5956 - val_footwear_output_acc: 0.6218 - val_pose_output_acc: 0.6122 - val_emotion_output_acc: 0.6706\n",
            "Epoch 48/50\n",
            "360/360 [==============================] - 127s 352ms/step - loss: 3.4693 - gender_output_loss: 0.5882 - image_quality_output_loss: 1.0744 - age_output_loss: 1.5044 - weight_output_loss: 1.0211 - bag_output_loss: 0.9290 - footwear_output_loss: 0.9037 - pose_output_loss: 0.9512 - emotion_output_loss: 1.1643 - gender_output_acc: 0.6926 - image_quality_output_acc: 0.4823 - age_output_acc: 0.3623 - weight_output_acc: 0.6088 - bag_output_acc: 0.5749 - footwear_output_acc: 0.5859 - pose_output_acc: 0.5821 - emotion_output_acc: 0.5604 - val_loss: 3.2017 - val_gender_output_loss: 0.5187 - val_image_quality_output_loss: 0.9977 - val_age_output_loss: 1.4143 - val_weight_output_loss: 0.9464 - val_bag_output_loss: 0.8682 - val_footwear_output_loss: 0.8375 - val_pose_output_loss: 0.8897 - val_emotion_output_loss: 1.0352 - val_gender_output_acc: 0.7420 - val_image_quality_output_acc: 0.5172 - val_age_output_acc: 0.4016 - val_weight_output_acc: 0.6393 - val_bag_output_acc: 0.5963 - val_footwear_output_acc: 0.6219 - val_pose_output_acc: 0.6105 - val_emotion_output_acc: 0.6726\n",
            "Epoch 49/50\n",
            "360/360 [==============================] - 127s 352ms/step - loss: 3.4801 - gender_output_loss: 0.5883 - image_quality_output_loss: 1.0847 - age_output_loss: 1.5098 - weight_output_loss: 1.0211 - bag_output_loss: 0.9392 - footwear_output_loss: 0.9056 - pose_output_loss: 0.9553 - emotion_output_loss: 1.1551 - gender_output_acc: 0.6912 - image_quality_output_acc: 0.4722 - age_output_acc: 0.3576 - weight_output_acc: 0.6045 - bag_output_acc: 0.5618 - footwear_output_acc: 0.5849 - pose_output_acc: 0.5832 - emotion_output_acc: 0.5622 - val_loss: 3.2039 - val_gender_output_loss: 0.5203 - val_image_quality_output_loss: 0.9976 - val_age_output_loss: 1.4149 - val_weight_output_loss: 0.9467 - val_bag_output_loss: 0.8688 - val_footwear_output_loss: 0.8376 - val_pose_output_loss: 0.8888 - val_emotion_output_loss: 1.0388 - val_gender_output_acc: 0.7405 - val_image_quality_output_acc: 0.5172 - val_age_output_acc: 0.4021 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5948 - val_footwear_output_acc: 0.6216 - val_pose_output_acc: 0.6113 - val_emotion_output_acc: 0.6710\n",
            "Epoch 50/50\n",
            "360/360 [==============================] - 127s 352ms/step - loss: 3.4775 - gender_output_loss: 0.5866 - image_quality_output_loss: 1.0814 - age_output_loss: 1.5048 - weight_output_loss: 1.0164 - bag_output_loss: 0.9391 - footwear_output_loss: 0.9136 - pose_output_loss: 0.9535 - emotion_output_loss: 1.1614 - gender_output_acc: 0.6905 - image_quality_output_acc: 0.4734 - age_output_acc: 0.3655 - weight_output_acc: 0.6098 - bag_output_acc: 0.5640 - footwear_output_acc: 0.5781 - pose_output_acc: 0.5853 - emotion_output_acc: 0.5636 - val_loss: 3.2030 - val_gender_output_loss: 0.5199 - val_image_quality_output_loss: 0.9972 - val_age_output_loss: 1.4146 - val_weight_output_loss: 0.9468 - val_bag_output_loss: 0.8688 - val_footwear_output_loss: 0.8375 - val_pose_output_loss: 0.8883 - val_emotion_output_loss: 1.0360 - val_gender_output_acc: 0.7410 - val_image_quality_output_acc: 0.5168 - val_age_output_acc: 0.4009 - val_weight_output_acc: 0.6398 - val_bag_output_acc: 0.5950 - val_footwear_output_acc: 0.6222 - val_pose_output_acc: 0.6124 - val_emotion_output_acc: 0.6720\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjwwORnWWI55",
        "colab_type": "code",
        "outputId": "8fbcc032-4cf5-4fb3-88b3-3cf1c54389c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "results = model.evaluate(test_gen, verbose=1)\n",
        "dict(zip(model.metrics_names, results))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 6s 88ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 0.3521825396825397,\n",
              " 'age_output_loss': 1.504874271059793,\n",
              " 'bag_output_acc': 0.5327380952380952,\n",
              " 'bag_output_loss': 0.9431826737191942,\n",
              " 'emotion_output_acc': 0.6622023809523809,\n",
              " 'emotion_output_loss': 1.0600457711825295,\n",
              " 'footwear_output_acc': 0.5838293650793651,\n",
              " 'footwear_output_loss': 0.8858748343255785,\n",
              " 'gender_output_acc': 0.685515873015873,\n",
              " 'gender_output_loss': 0.5902978607586452,\n",
              " 'image_quality_output_acc': 0.5143849206349206,\n",
              " 'image_quality_output_loss': 1.0075190833636694,\n",
              " 'loss': 3.4378237156640914,\n",
              " 'pose_output_acc': 0.591765873015873,\n",
              " 'pose_output_loss': 0.9271977411376106,\n",
              " 'weight_output_acc': 0.6235119047619048,\n",
              " 'weight_output_loss': 1.0226027747941395}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}